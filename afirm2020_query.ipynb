{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "URfNSXe6L8sH"
   },
   "source": [
    "# Querying\n",
    "\n",
    "In this exercise, we are going to first interactively query the index and then produce a TREC run with [Pyserini](https://github.com/castorini/pyserini), the Python interface to Anserini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m1y2srQHdOA3"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hoP7jph2d78z"
   },
   "source": [
    "Install Python dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IFGfQRYFMO2A"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install pyjnius==1.2.1\n",
    "!pip install pyserini\n",
    "\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z0HQz3QLd-9_"
   },
   "source": [
    "Fix known issue with pyjnius (see [this explanation](https://github.com/castorini/pyserini/blob/master/README.md#known-issues) for details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eKT2WWp2eEJy"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!mkdir -p /usr/lib/jvm/java-1.11.0-openjdk-amd64/jre/lib/amd64/server/\n",
    "!ln -s /usr/lib/jvm/java-1.11.0-openjdk-amd64/lib/server/libjvm.so /usr/lib/jvm/java-1.11.0-openjdk-amd64/jre/lib/amd64/server/libjvm.so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lnqt7w1sU4Dv"
   },
   "source": [
    "Let's pull the Anserini jar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h4xSZLdsVS3-"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!gsutil -m cp gs://afirm2020/anserini-0.7.1-SNAPSHOT-fatjar.jar ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Wcr7BpfVgSg"
   },
   "source": [
    "Let's point Pyserini to the Anserini jar that we have just pulled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VuAwdnkpViBH"
   },
   "outputs": [],
   "source": [
    "os.environ['ANSERINI_CLASSPATH'] = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rsEU-U3ketRL"
   },
   "source": [
    "## Interactive Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zK1D24DDTWcO"
   },
   "source": [
    "Pull the pre-built index from GCS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HU6Vyp4PTY7p"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!gsutil -m cp -r gs://afirm2020/indexes ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wokA-RljLy0V"
   },
   "outputs": [],
   "source": [
    "from pyserini.search import pysearch\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FO8CncEOcbjq"
   },
   "source": [
    "First, let's see grab the queries that are defined for our collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oY0Ls72U3GSc"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!mkdir data\n",
    "!gsutil -m cp gs://afirm2020/msmarco_passage/queries.dev.small.tsv data/queries.dev.small.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4xMlOOqhe5xp"
   },
   "source": [
    "The hits data structure holds the docid, the retrieval score, as well as the document content.\n",
    "Let's look at the top 10 passages for the query `south african football teams`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 967
    },
    "colab_type": "code",
    "id": "IpsnrIaoMYg_",
    "outputId": "d061aa4f-50dc-44d3-8082-9e921a9a1680"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "searcher = pysearch.SimpleSearcher('indexes/lucene-index.msmarco-passage.pos+docvectors+rawdocs')\n",
    "interactive_hits = searcher.search('south african football teams')\n",
    "\n",
    "for i in range(0, 10):\n",
    "    print('Rank: {} | Passage ID: {} | BM25 Score: {}'.format(i+1, interactive_hits[i].docid, interactive_hits[i].score))\n",
    "    display(HTML('<div style=\"font-family: Times New Roman; padding-bottom:10px\">' + interactive_hits[i].content + '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ogrmRRfobshT"
   },
   "source": [
    "The above example uses default parameters.\n",
    "Let's try setting tuned parameters for this collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 967
    },
    "colab_type": "code",
    "id": "XIP34ZtsCaB5",
    "outputId": "008072c9-c46a-44b7-d0c3-d9eed16c1367"
   },
   "outputs": [],
   "source": [
    "searcher.set_bm25_similarity(0.82, 0.68)\n",
    "interactive_hits_tuned = searcher.search('south african football teams')\n",
    "\n",
    "for i in range(0, 10):\n",
    "    print('Rank: {} | Passage ID: {} | BM25 Score: {}'.format(i+1, interactive_hits_tuned[i].docid, interactive_hits_tuned[i].score))\n",
    "    display(HTML('<div style=\"font-family: Times New Roman; padding-bottom:10px\">' + interactive_hits[i].content + '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJpEXdsx3uBJ"
   },
   "source": [
    "**Exercise:**\n",
    "Compare the rankings with and without tuned parameters.\n",
    "Add a new cell to query the index with a different query of your choice, both with untuned and tuned parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mdZeRTf3ClAR"
   },
   "source": [
    "Note how the ranking has changed.\n",
    "We can also enable RM3 query expansion to see if it helps with our collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 950
    },
    "colab_type": "code",
    "id": "LH3EVKaTMcgg",
    "outputId": "c153d4af-c0a7-4cc6-a7af-4ea919b47aff"
   },
   "outputs": [],
   "source": [
    "searcher.set_rm3_reranker(10, 10, 0.5)\n",
    "interactive_hits_tuned_rm3 = searcher.search('south african football teams')\n",
    "\n",
    "for i in range(0, 10):\n",
    "    print('Rank: {} | Passage ID: {} | BM25 Score: {}'.format(i+1, interactive_hits_tuned_rm3[i].docid, interactive_hits_tuned_rm3[i].score))\n",
    "    display(HTML('<div style=\"font-family: Times New Roman; padding-bottom:10px\">' + interactive_hits_tuned_rm3[i].content + '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fHX_l-5jKZ3P"
   },
   "source": [
    "## Batch Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k08tEtsmDCyv"
   },
   "source": [
    "Previously we interactively queried the index.\n",
    "However, in a typical experimental setting, you would evaluate over a larger number of queries to test different information needs.\n",
    "\n",
    "Let's begin by constructing the dev queries and corresponding query IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "RSlbbOMecd2d",
    "outputId": "ba8c3804-b4af-44b4-e6fb-17f7ceb0e53c"
   },
   "outputs": [],
   "source": [
    "topics = {}\n",
    "with open('data/queries.dev.small.tsv') as file:\n",
    "    for line in file:\n",
    "       id, q = line.strip().split('\\t')\n",
    "       topics[int(id)] = q\n",
    "\n",
    "print('{} queries total'.format(len(topics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LpSWU35Bd9Bp"
   },
   "outputs": [],
   "source": [
    "queries = list(topics.values())\n",
    "qids = list([str(t) for t in topics.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mtYICC936IMU"
   },
   "source": [
    "**Exercise:**\n",
    "We have previously looked at these queries in the previous activity.\n",
    "Again find the queries that contain `football`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "fW6cX15t5cGd",
    "outputId": "ddcdba58-5a01-4b6f-b6d5-35a837b5ab69"
   },
   "outputs": [],
   "source": [
    "[q for q in queries if 'football' in q]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F37KoOfsfCSe"
   },
   "source": [
    "Now, let's run all the queries from the dev set.\n",
    "For the sake of speed, let's again only retrieve the top 10 documents for each query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C74qogSKfFtD"
   },
   "outputs": [],
   "source": [
    "searcher = pysearch.SimpleSearcher('indexes/lucene-index.msmarco-passage.pos+docvectors+rawdocs')\n",
    "bm25_hits = searcher.batch_search(queries, qids, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0HTohrkIO5N2"
   },
   "source": [
    "Note that the above runs batch retrieval with untuned BM25.\n",
    "We can repeat with tuned parameters, just like we did for the interactive queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R6clfMWkD3ZU"
   },
   "outputs": [],
   "source": [
    "searcher.set_bm25_similarity(0.82, 0.68)\n",
    "bm25_hits_tuned = searcher.batch_search(queries, qids, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mNMa1DKsD1fc"
   },
   "source": [
    "Now let's repeat with RM3 query expansion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j9o_IXRTk5mf"
   },
   "outputs": [],
   "source": [
    "searcher.set_rm3_reranker(10, 10, 0.5)\n",
    "bm25_hits_tuned_rm3 = searcher.batch_search(queries, qids, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6J-CuGlz9hjc"
   },
   "source": [
    "**Exercise:**\n",
    "So far we have downloaded and retrieved the top passages for the dev queries.\n",
    "Now pull the eval queries and repeat the process for eval queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "7npsIcEU91Ji",
    "outputId": "1faddb44-0303-4d1e-f5d7-29bf6bdb1811"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!gsutil -m cp gs://afirm2020/msmarco_passage/queries.eval.small.tsv data/queries.eval.small.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRxS0rzG--tB"
   },
   "outputs": [],
   "source": [
    "eval_topics = {}\n",
    "with open('data/queries.eval.small.tsv') as file:\n",
    "    for line in file:\n",
    "       id, q = line.strip().split('\\t')\n",
    "       eval_topics[int(id)] = q\n",
    "\n",
    "eval_queries = list(eval_topics.values())\n",
    "eval_qids = list([str(t) for t in eval_topics.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nYeEGpfs_myX"
   },
   "outputs": [],
   "source": [
    "eval_searcher = pysearch.SimpleSearcher('indexes/lucene-index.msmarco-passage.pos+docvectors+rawdocs')\n",
    "bm25_hits = eval_searcher.batch_search(eval_queries, eval_qids, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tU1jjvYfKYgA"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "A crucial component of information retrieval research is evaluation and metrics.\n",
    "The most common tool used to achieve this goal is `trec_eval` developed by [NIST](https://www.nist.gov/).\n",
    "\n",
    "`trec_eval` defines a number of standard retrieval measures, the details of which can be seen [here](http://www.rafaelglater.com/en/post/learn-how-to-use-trec_eval-to-evaluate-your-information-retrieval-system).\n",
    "\n",
    "### TREC Format\n",
    "\n",
    "`trec_eval` requires the runs from various experiments to be expressed in a standard TREC format:\n",
    "\n",
    "`query_id iter docno rank similarity run_id` delimited by spaces\n",
    "\n",
    "- `query_id`: query ID\n",
    "- `iter`: constant, often either 0 or Q0 - required but ignored by `trec_eval`\n",
    "- `docno`: string values that uniquely identify a document in the collection\n",
    "- `rank`: integer, often zero indexed\n",
    "- `similarity`: float value that represents the similarity of the document to the query specified by `query_id`\n",
    "- `run_id`: string that identifies runs, used to keep track of different experiments - also ignored by `trec_eval`\n",
    "\n",
    "Evaluation also requires the ground truth in the form of relevance judgements in the qrels file.\n",
    "The qrels file follows the following format:\n",
    "\n",
    "`query_id iter docno label`\n",
    "\n",
    "- `label`: binary code (0 for not relevant and 1 for relevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GnhjuvsQhrVB"
   },
   "source": [
    "Convert the hits for both BM25 (tuned and untuned) and BM25+RM3 into the TREC format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_I_F6K9kW3L"
   },
   "outputs": [],
   "source": [
    "def convert_to_trec_run(experiment, run_dict):\n",
    "  with open('run.{}.txt'.format(experiment), 'w') as run_file:\n",
    "    for qid in run_dict:\n",
    "      for rank, doc in enumerate(run_dict[qid]):\n",
    "        run_file.write('{} Q0 {} {} {} {}\\n'.format(qid, doc.docid, rank, doc.score, experiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nDPIwvy5iqMK"
   },
   "outputs": [],
   "source": [
    "convert_to_trec_run('msmarco_passage_dev_bm25', bm25_hits)\n",
    "convert_to_trec_run('msmarco_passage_dev_bm25_tuned', bm25_hits_tuned)\n",
    "convert_to_trec_run('msmarco_passage_dev_bm25_tuned_rm3', bm25_hits_tuned_rm3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NtKO5XXWnWZi"
   },
   "source": [
    "Let's pull `trec_eval` and the qrels file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SQWfpv1gnYaf"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!gsutil -m cp -r gs://afirm2020/trec_eval.9.0.4 .\n",
    "!chmod -R +x trec_eval.9.0.4/\n",
    "!gsutil -m cp gs://afirm2020/msmarco_passage/qrels.dev.small.tsv data/qrels.dev.small.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J8dRwjl3aGsn"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MmWLvAl7iqbz"
   },
   "source": [
    "Now that we have our runs in the TREC format, we can evaluate them with `trec_eval`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "5QWvGHiXVx0V",
    "outputId": "ebb9ad86-b59b-4e72-e3bb-7cc2116d57b0"
   },
   "outputs": [],
   "source": [
    "!head -5 run.msmarco_passage_dev_bm25.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PPNjxxLWO0X-",
    "outputId": "b64ae765-5901-4c27-ea38-5a4a6e1d2d29"
   },
   "outputs": [],
   "source": [
    "!trec_eval.9.0.4/trec_eval -m map -c -m recall.1000 -c data/qrels.dev.small.tsv run.msmarco_passage_dev_bm25.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "XXWy1WHZGq_Y",
    "outputId": "7b076138-88bb-44fb-e250-75a75719a760"
   },
   "outputs": [],
   "source": [
    "!chmod -R +x trec_eval.9.0.4/\n",
    "!trec_eval.9.0.4/trec_eval -m map -c -m recall.1000 -c data/qrels.dev.small.tsv run.msmarco_passage_dev_bm25_tuned.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "xQV_2o-OoDkv",
    "outputId": "422d7a6e-652e-4ad4-fb86-73289d9df120"
   },
   "outputs": [],
   "source": [
    "!trec_eval.9.0.4/trec_eval -m map -c -m recall.1000 -c data/qrels.dev.small.tsv run.msmarco_passage_dev_bm25_tuned_rm3.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DdX1XQfKG7QB"
   },
   "source": [
    "*TODO: comments and comparisons*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uedB2S4jY9Ka"
   },
   "source": [
    "**Exercise:**\n",
    "We obtained the run file for the eval queries in the previous exercise.\n",
    "Now evaluate it with `trec_tool`."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "afirm2020-query.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
