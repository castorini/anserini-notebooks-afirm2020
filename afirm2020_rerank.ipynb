{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/castorini/anserini-notebooks-afirm2020/blob/master/afirm2020_rerank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzvtO91DqY1G",
        "colab_type": "text"
      },
      "source": [
        "# Re-ranking with BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKNfJI3vqoEi",
        "colab_type": "text"
      },
      "source": [
        "In this activity, we are going to first learn how to re-rank with BERT and then compare the retrieval effectiveness to BM25.\n",
        "This is a rather lengthy activity, so consider continuing in your own time if we run out of time in the tutorial.\n",
        "The intermediate models are saved regularly, which makes it possible to stop and resume at any point.\n",
        "\n",
        "We are going to run a [BERT](https://arxiv.org/abs/1810.04805)-based re-ranker over our candidate MS MARCO passages.\n",
        "BERT is a massively pretrained language model that has been shown to improve many downstream NLP tasks.\n",
        "It is highly recommended that you check out the [original paper](https://arxiv.org/abs/1901.04085) on using BERT for passage re-ranking before you proceed with this exercise.\n",
        "\n",
        "This notebook is based on the [dl4marco-bert](https://github.com/nyu-dl/dl4marco-bert) repo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5N7IU7zl1UI",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Firstly, we need to set up Colab TPU running environment, verify a TPU device is succesfully connected and upload credentials to TPU for GCS bucket usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --upgrade tensorflow==1.15"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET8rn1TNhrlc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZDNgAEdmRUR",
        "colab_type": "text"
      },
      "source": [
        "Secondly, clone the `dl4marco-bert` repo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USzW4PWnmQqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "!test -d bert_repo || git clone https://github.com/nyu-dl/dl4marco-bert dl4marco-bert\n",
        "if not 'dl4marco-bert' in sys.path:\n",
        "  sys.path += ['dl4marco-bert']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVrrnw6Jmj_N",
        "colab_type": "text"
      },
      "source": [
        "## Data\n",
        "\n",
        "The TensorFlow implementation for passage re-ranking expects the data in TFRecord format.\n",
        "Since generating these files from the original collection takes 1-2 days, we have already made them available on GCS.\n",
        "\n",
        "- `BERT_MODEL`: Uncased Base or Large model (we recommend starting with Base as it takes less time to run)\n",
        "- `OUTPUT_DIR`: The output path in the GCS filesystem to store model checkpoints and evaluation results\n",
        "- `DATA_DIR`: The data path in the GCS filesystem (contains the TFRecord)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp9i52ltvkAV",
        "colab_type": "text"
      },
      "source": [
        "You need a unique `OUTPUT_DIR` in the bucket, so first create a folder with your name.\n",
        "`gsutil` doesn't allow the direct creation of folders; instead, create a dummy file in your folder, and move it to GCS.\n",
        "Also make sure you update the value of the `OUTPUT_DIR` variable with your folder name blow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-yWCcjCvx2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir unique_name\n",
        "!touch unique_name/dummy.txt\n",
        "!gsutil cp -r unique_name gs://sigir2020"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UtHASdOmryD",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# Available pretrained model checkpoints:\n",
        "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
        "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
        "BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "BERT_PRETRAINED_DIR = 'gs://cloud-tpu-checkpoints/bert/' + BERT_MODEL\n",
        "print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n",
        "!gsutil ls $BERT_PRETRAINED_DIR\n",
        "\n",
        "OUTPUT_DIR = 'gs://sigir2020/unique_name' #@param {type:\"string\"}\n",
        "assert OUTPUT_DIR, 'Must specify an existing GCS bucket name'\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n",
        "\n",
        "# Now we need to specify the input data dir. Should contain the .tfrecord files \n",
        "# and the supporting query-docids mapping files.\n",
        "DATA_DIR = 'gs://sigir2020/tfrecord' #@param {type:\"string\"}\n",
        "print('***** Data directory: {} *****'.format(DATA_DIR))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82qY9OMDo2Sg",
        "colab_type": "text"
      },
      "source": [
        "## Training / Evaluation\n",
        "\n",
        "With all our data and models in place, we can now train BERT for passage re-ranking and evaluate on the dev set.\n",
        "\n",
        "Make sure to set `DO_TRAIN` to run training and `DO_EVAL` to evaluate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNFKSsDBvJuy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters\n",
        "USE_TPU = True\n",
        "DO_TRAIN = True\n",
        "DO_EVAL = False\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "EVAL_BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-6\n",
        "NUM_TRAIN_STEPS = 400000\n",
        "NUM_WARMUP_STEPS = 40000\n",
        "MAX_SEQ_LENGTH = 512\n",
        "SAVE_CHECKPOINTS_STEPS = 1000\n",
        "ITERATIONS_PER_LOOP = 1000\n",
        "NUM_TPU_CORES = 8\n",
        "BERT_CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
        "INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
        "MSMARCO_OUTPUT = True  # Write the predictions to a MS-MARCO-formatted file.\n",
        "MAX_EVAL_EXAMPLES = None  # Maximum number of examples to be evaluated.\n",
        "NUM_EVAL_DOCS = 1000  # Number of docs per query in the dev and eval files.\n",
        "METRICS_MAP = ['MAP', 'RPrec', 'NDCG', 'MRR', 'MRR@10']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y_aQMUUGqMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import metrics\n",
        "import modeling\n",
        "import optimization\n",
        "\n",
        "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
        "                 labels, num_labels, use_one_hot_embeddings):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "  model = modeling.BertModel(\n",
        "      config=bert_config,\n",
        "      is_training=is_training,\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      token_type_ids=segment_ids,\n",
        "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "  output_layer = model.get_pooled_output()\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "    if is_training:\n",
        "      # I.e., 0.1 dropout\n",
        "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "    return (loss, per_example_loss, log_probs)\n",
        "\n",
        "\n",
        "def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n",
        "                     num_train_steps, num_warmup_steps, use_tpu,\n",
        "                     use_one_hot_embeddings):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    (total_loss, per_example_loss, log_probs) = create_model(\n",
        "        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
        "        num_labels, use_one_hot_embeddings)\n",
        "\n",
        "    tvars = tf.trainable_variables()\n",
        "\n",
        "    scaffold_fn = None\n",
        "    initialized_variable_names = []\n",
        "    if init_checkpoint:\n",
        "      (assignment_map, initialized_variable_names\n",
        "      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "      if use_tpu:\n",
        "        def tpu_scaffold():\n",
        "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "          return tf.train.Scaffold()\n",
        "\n",
        "        scaffold_fn = tpu_scaffold\n",
        "      else:\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    tf.logging.info(\"**** Trainable Variables ****\")\n",
        "    for var in tvars:\n",
        "      init_string = \"\"\n",
        "      if var.name in initialized_variable_names:\n",
        "        init_string = \", *INIT_FROM_CKPT*\"\n",
        "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "                      init_string)\n",
        "\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "\n",
        "      train_op = optimization.create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          train_op=train_op,\n",
        "          scaffold_fn=scaffold_fn)\n",
        "\n",
        "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          predictions={\n",
        "              \"log_probs\": log_probs,\n",
        "              \"label_ids\": label_ids,\n",
        "          },\n",
        "          scaffold_fn=scaffold_fn)\n",
        "\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          \"Only TRAIN and PREDICT modes are supported: %s\" % (mode))\n",
        "\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn\n",
        "\n",
        "\n",
        "def input_fn_builder(dataset_path, seq_length, is_training,\n",
        "                     max_eval_examples=None):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "\n",
        "    batch_size = params[\"batch_size\"]\n",
        "    output_buffer_size = batch_size * 1000\n",
        "\n",
        "    def extract_fn(data_record):\n",
        "      features = {\n",
        "          \"query_ids\": tf.FixedLenSequenceFeature(\n",
        "              [], tf.int64, allow_missing=True),\n",
        "          \"doc_ids\": tf.FixedLenSequenceFeature(\n",
        "              [], tf.int64, allow_missing=True),\n",
        "          \"label\": tf.FixedLenFeature([], tf.int64),\n",
        "      }\n",
        "      sample = tf.parse_single_example(data_record, features)\n",
        "\n",
        "      query_ids = tf.cast(sample[\"query_ids\"], tf.int32)\n",
        "      doc_ids = tf.cast(sample[\"doc_ids\"], tf.int32)\n",
        "      label_ids = tf.cast(sample[\"label\"], tf.int32)\n",
        "      input_ids = tf.concat((query_ids, doc_ids), 0)\n",
        "\n",
        "      query_segment_id = tf.zeros_like(query_ids)\n",
        "      doc_segment_id = tf.ones_like(doc_ids)\n",
        "      segment_ids = tf.concat((query_segment_id, doc_segment_id), 0)\n",
        "\n",
        "      input_mask = tf.ones_like(input_ids)\n",
        "\n",
        "      features = {\n",
        "          \"input_ids\": input_ids,\n",
        "          \"segment_ids\": segment_ids,\n",
        "          \"input_mask\": input_mask,\n",
        "          \"label_ids\": label_ids,\n",
        "      }\n",
        "      return features\n",
        "\n",
        "    dataset = tf.data.TFRecordDataset([dataset_path])\n",
        "    dataset = dataset.map(\n",
        "        extract_fn, num_parallel_calls=4).prefetch(output_buffer_size)\n",
        "\n",
        "    if is_training:\n",
        "      dataset = dataset.repeat()\n",
        "      dataset = dataset.shuffle(buffer_size=1000)\n",
        "    else:\n",
        "      if max_eval_examples:\n",
        "        # Use at most this number of examples (debugging only).\n",
        "        dataset = dataset.take(max_eval_examples)\n",
        "        # pass\n",
        "\n",
        "    dataset = dataset.padded_batch(\n",
        "        batch_size=batch_size,\n",
        "        padded_shapes={\n",
        "            \"input_ids\": [seq_length],\n",
        "            \"segment_ids\": [seq_length],\n",
        "            \"input_mask\": [seq_length],\n",
        "            \"label_ids\": [],\n",
        "        },\n",
        "        padding_values={\n",
        "            \"input_ids\": 0,\n",
        "            \"segment_ids\": 0,\n",
        "            \"input_mask\": 0,\n",
        "            \"label_ids\": 0,\n",
        "        },\n",
        "        drop_remainder=True)\n",
        "\n",
        "    return dataset\n",
        "  return input_fn\n",
        "\n",
        "\n",
        "def main(_):\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  \n",
        "  if not DO_TRAIN and not DO_EVAL:\n",
        "    raise ValueError(\"At least one of `DO_TRAIN` or `DO_EVAL` must be True.\")\n",
        "\n",
        "  bert_config = modeling.BertConfig.from_json_file(BERT_CONFIG_FILE)\n",
        "\n",
        "  if MAX_SEQ_LENGTH > bert_config.max_position_embeddings:\n",
        "    raise ValueError(\n",
        "        \"Cannot use sequence length %d because the BERT model \"\n",
        "        \"was only trained up to sequence length %d\" %\n",
        "        (MAX_SEQ_LENGTH, bert_config.max_position_embeddings))\n",
        "\n",
        "  tpu_cluster_resolver = None\n",
        "  if USE_TPU:\n",
        "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
        "        TPU_ADDRESS)\n",
        "\n",
        "  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
        "  run_config = tf.contrib.tpu.RunConfig(\n",
        "      cluster=tpu_cluster_resolver,\n",
        "      model_dir=OUTPUT_DIR,\n",
        "      save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "          iterations_per_loop=ITERATIONS_PER_LOOP,\n",
        "          num_shards=NUM_TPU_CORES,\n",
        "          per_host_input_for_training=is_per_host))\n",
        "\n",
        "  model_fn = model_fn_builder(\n",
        "      bert_config=bert_config,\n",
        "      num_labels=2,\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      num_train_steps=NUM_TRAIN_STEPS,\n",
        "      num_warmup_steps=NUM_WARMUP_STEPS,\n",
        "      use_tpu=USE_TPU,\n",
        "      use_one_hot_embeddings=USE_TPU)\n",
        "\n",
        "  # If TPU is not available, this will fall back to normal Estimator on CPU\n",
        "  # or GPU.\n",
        "  estimator = tf.contrib.tpu.TPUEstimator(\n",
        "      use_tpu=USE_TPU,\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      train_batch_size=TRAIN_BATCH_SIZE,\n",
        "      eval_batch_size=EVAL_BATCH_SIZE,\n",
        "      predict_batch_size=EVAL_BATCH_SIZE)\n",
        "\n",
        "  if DO_TRAIN:\n",
        "    tf.logging.info(\"***** Running training *****\")\n",
        "    tf.logging.info(\"  Batch size = %d\", TRAIN_BATCH_SIZE)\n",
        "    tf.logging.info(\"  Num steps = %d\", NUM_TRAIN_STEPS)\n",
        "    train_input_fn = input_fn_builder(\n",
        "        dataset_path=DATA_DIR + \"/dataset_train.tf\",\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=True)\n",
        "    estimator.train(input_fn=train_input_fn,\n",
        "                    max_steps=NUM_TRAIN_STEPS)\n",
        "    tf.logging.info(\"Done Training!\")\n",
        "\n",
        "  if DO_EVAL:\n",
        "    for set_name in [\"dev\", \"eval\"]:\n",
        "      tf.logging.info(\"***** Running evaluation *****\")\n",
        "      tf.logging.info(\"  Batch size = %d\", EVAL_BATCH_SIZE)\n",
        "      max_eval_examples = None\n",
        "      if MAX_EVAL_EXAMPLES:\n",
        "        max_eval_examples = MAX_EVAL_EXAMPLES * NUM_EVAL_DOCS\n",
        "\n",
        "      eval_input_fn = input_fn_builder(\n",
        "          dataset_path=DATA_DIR + \"/dataset_\" + set_name + \".tf\",\n",
        "          seq_length=MAX_SEQ_LENGTH,\n",
        "          is_training=False,\n",
        "          max_eval_examples=max_eval_examples)\n",
        "\n",
        "      if MSMARCO_OUTPUT:\n",
        "        msmarco_file = tf.gfile.Open(\n",
        "            OUTPUT_DIR + \"/msmarco_predictions_\" + set_name + \".tsv\", \"w\")\n",
        "        query_docids_map = []\n",
        "        with tf.gfile.Open(\n",
        "            DATA_DIR + \"/query_doc_ids_\" + set_name + \".txt\") as ref_file:\n",
        "          for line in ref_file:\n",
        "            query_docids_map.append(line.strip().split(\"\\t\"))\n",
        "\n",
        "      # ***IMPORTANT NOTE***\n",
        "      # The logging output produced by the feed queues during evaluation is very \n",
        "      # large (~14M lines for the dev set), which causes the tab to crash if you\n",
        "      # don't have enough memory on your local machine. We suppress this \n",
        "      # frequent logging by setting the verbosity to WARN during the evaluation\n",
        "      # phase.\n",
        "      tf.logging.set_verbosity(tf.logging.WARN)\n",
        "      \n",
        "      result = estimator.predict(input_fn=eval_input_fn,\n",
        "                                 yield_single_examples=True)\n",
        "      start_time = time.time()\n",
        "      results = []\n",
        "      all_metrics = np.zeros(len(METRICS_MAP))\n",
        "      example_idx = 0\n",
        "      total_count = 0\n",
        "      for item in result:\n",
        "        results.append((item[\"log_probs\"], item[\"label_ids\"]))\n",
        "\n",
        "        if len(results) == NUM_EVAL_DOCS:\n",
        "\n",
        "          log_probs, labels = zip(*results)\n",
        "          log_probs = np.stack(log_probs).reshape(-1, 2)\n",
        "          labels = np.stack(labels)\n",
        "\n",
        "          scores = log_probs[:, 1]\n",
        "          pred_docs = scores.argsort()[::-1]\n",
        "          gt = set(list(np.where(labels > 0)[0]))\n",
        "\n",
        "          all_metrics += metrics.metrics(\n",
        "              gt=gt, pred=pred_docs, metrics_map=METRICS_MAP)\n",
        "\n",
        "          if MSMARCO_OUTPUT:\n",
        "            start_idx = example_idx * NUM_EVAL_DOCS\n",
        "            end_idx = (example_idx + 1) * NUM_EVAL_DOCS\n",
        "            query_ids, doc_ids = zip(*query_docids_map[start_idx:end_idx])\n",
        "            assert len(set(query_ids)) == 1, \"Query ids must be all the same.\"\n",
        "            query_id = query_ids[0]\n",
        "            rank = 1\n",
        "            for doc_idx in pred_docs:\n",
        "              doc_id = doc_ids[doc_idx]\n",
        "              # Skip fake docs, as they are only used to ensure that each query\n",
        "              # has 1000 docs.\n",
        "              if doc_id != \"00000000\":\n",
        "                msmarco_file.write(\n",
        "                    \"\\t\".join((query_id, doc_id, str(rank))) + \"\\n\")\n",
        "                rank += 1\n",
        "\n",
        "          example_idx += 1\n",
        "          results = []\n",
        "\n",
        "        total_count += 1\n",
        "\n",
        "        if total_count % 10000 == 0:\n",
        "          tf.logging.warn(\"Read {} examples in {} secs. Metrics so far:\".format(\n",
        "              total_count, int(time.time() - start_time)))\n",
        "          tf.logging.warn(\"  \".join(METRICS_MAP))\n",
        "          tf.logging.warn(all_metrics / example_idx)\n",
        "\n",
        "      # Once the feed queues are finished, we can set the verbosity back to \n",
        "      # INFO.\n",
        "      tf.logging.set_verbosity(tf.logging.INFO)\n",
        "      \n",
        "      if MSMARCO_OUTPUT:\n",
        "        msmarco_file.close()\n",
        "\n",
        "      all_metrics /= example_idx\n",
        "\n",
        "      tf.logging.info(\"Eval {}:\".format(set_name))\n",
        "      tf.logging.info(\"  \".join(METRICS_MAP))\n",
        "      tf.logging.info(all_metrics)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  tf.app.run()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "afirm2020_rerank.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}